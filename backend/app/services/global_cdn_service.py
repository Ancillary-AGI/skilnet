"""
Global CDN Integration - Multi-Region Content Delivery
Superior to basic static file serving with intelligent caching and global distribution
"""

import asyncio
import logging
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta
import uuid
import hashlib
import json
import os
from pathlib import Path
import tempfile
from dataclasses import dataclass, field
from enum import Enum
import mimetypes
import gzip
import brotli

import boto3
import requests
from fastapi.responses import FileResponse, StreamingResponse
import aiofiles
import aiohttp

logger = logging.getLogger(__name__)


class CDNProvider(Enum):
    """Supported CDN providers"""
    CLOUDFRONT = "cloudfront"
    CLOUDFLARE = "cloudflare"
    AZURE_CDN = "azure_cdn"
    GOOGLE_CDN = "google_cdn"
    AKAMAI = "akamai"


class ContentType(Enum):
    """Content types for optimization"""
    STATIC_ASSETS = "static_assets"
    USER_UPLOADS = "user_uploads"
    COURSE_CONTENT = "course_content"
    VIDEO_STREAMING = "video_streaming"
    VR_AR_ASSETS = "vr_ar_assets"
    AI_GENERATED = "ai_generated"


@dataclass
class CDNEndpoint:
    """CDN endpoint configuration"""
    endpoint_id: str
    provider: CDNProvider
    region: str
    base_url: str
    access_key: str
    secret_key: str
    bucket_name: Optional[str] = None
    distribution_id: Optional[str] = None
    is_active: bool = True
    created_at: datetime = field(default_factory=datetime.utcnow)


@dataclass
class ContentMetadata:
    """Metadata for cached content"""
    content_id: str
    original_path: str
    cdn_urls: Dict[str, str]  # region -> cdn_url
    content_type: ContentType
    file_size: int
    mime_type: str
    checksum: str
    compression: List[str]  # gzip, brotli, etc.
    cache_headers: Dict[str, str]
    uploaded_at: datetime
    expires_at: Optional[datetime] = None


@dataclass
class CDNMetrics:
    """CDN performance metrics"""
    endpoint_id: str
    timestamp: datetime
    requests_total: int
    requests_per_second: float
    bandwidth_used_gb: float
    cache_hit_rate: float
    average_latency_ms: float
    error_rate: float
    region: str


class GlobalCDNService:
    """
    Advanced global CDN service that surpasses basic file serving

    Features:
    - Multi-region content distribution
    - Intelligent caching strategies
    - Automatic content optimization
    - Real-time performance monitoring
    - Failover and redundancy
    - Edge computing capabilities
    - Dynamic content acceleration
    """

    def __init__(self):
        self.endpoints: Dict[str, CDNEndpoint] = {}
        self.content_metadata: Dict[str, ContentMetadata] = {}
        self.metrics_data: Dict[str, List[CDNMetrics]] = {}
        self.cache_strategy: Dict[ContentType, Dict[str, Any]] = {}

        # Initialize cache strategies for different content types
        self._initialize_cache_strategies()

        # Performance tracking
        self.request_count = 0
        self.bandwidth_used = 0
        self.cache_hits = 0
        self.cache_misses = 0

    async def initialize(self):
        """Initialize CDN service and endpoints"""
        await self._initialize_cdn_endpoints()
        await self._setup_content_optimization()
        logger.info("ðŸš€ Global CDN service initialized")

    async def upload_content(
        self,
        file_path: str,
        content_type: ContentType,
        custom_filename: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> str:
        """Upload content to global CDN"""

        content_id = str(uuid.uuid4())

        try:
            # Read file
            file_stats = os.stat(file_path)
            file_size = file_stats.st_size

            # Generate checksum
            checksum = await self._generate_file_checksum(file_path)

            # Determine MIME type
            mime_type, _ = mimetypes.guess_type(file_path)
            if not mime_type:
                mime_type = "application/octet-stream"

            # Generate CDN URLs for all regions
            cdn_urls = {}
            upload_tasks = []

            for endpoint_id, endpoint in self.endpoints.items():
                if endpoint.is_active:
                    task = self._upload_to_endpoint(
                        file_path, endpoint, content_id, custom_filename, content_type
                    )
                    upload_tasks.append((endpoint_id, task))

            # Upload to all endpoints in parallel
            for endpoint_id, task in upload_tasks:
                try:
                    cdn_url = await task
                    cdn_urls[endpoint.region] = cdn_url
                except Exception as e:
                    logger.error(f"âŒ Failed to upload to {endpoint_id}: {e}")

            # Create content metadata
            content_metadata = ContentMetadata(
                content_id=content_id,
                original_path=file_path,
                cdn_urls=cdn_urls,
                content_type=content_type,
                file_size=file_size,
                mime_type=mime_type,
                checksum=checksum,
                compression=self._get_compression_methods(file_path),
                cache_headers=self._generate_cache_headers(content_type),
                uploaded_at=datetime.utcnow(),
                expires_at=datetime.utcnow() + timedelta(days=365)  # 1 year default
            )

            self.content_metadata[content_id] = content_metadata

            # Store metadata in database
            await self._store_metadata_in_db(content_metadata)

            logger.info(f"ðŸ“¦ Content uploaded to CDN: {content_id}")
            return content_id

        except Exception as e:
            logger.error(f"âŒ Failed to upload content: {e}")
            raise

    async def get_content_url(
        self,
        content_id: str,
        user_region: str = "auto",
        preferred_provider: Optional[CDNProvider] = None
    ) -> str:
        """Get optimal CDN URL for user"""

        if content_id not in self.content_metadata:
            raise ValueError(f"Content {content_id} not found")

        metadata = self.content_metadata[content_id]

        # Auto-detect user region if not specified
        if user_region == "auto":
            user_region = await self._detect_user_region()

        # Find best endpoint for user region
        best_url = await self._select_optimal_endpoint(
            metadata.cdn_urls, user_region, preferred_provider
        )

        if not best_url:
            # Fallback to any available URL
            best_url = list(metadata.cdn_urls.values())[0]

        # Update access metrics
        await self._record_content_access(content_id, user_region)

        return best_url

    async def optimize_content(
        self,
        content_id: str,
        optimization_type: str = "auto"
    ) -> Dict[str, Any]:
        """Optimize content for better delivery"""

        if content_id not in self.content_metadata:
            raise ValueError(f"Content {content_id} not found")

        metadata = self.content_metadata[content_id]

        optimizations = {
            "compression_applied": [],
            "image_optimization": {},
            "video_optimization": {},
            "cache_optimization": {}
        }

        # Apply compression
        if "gzip" not in metadata.compression:
            await self._apply_compression(content_id, "gzip")
            optimizations["compression_applied"].append("gzip")

        if "brotli" not in metadata.compression:
            await self._apply_compression(content_id, "brotli")
            optimizations["compression_applied"].append("brotli")

        # Content-type specific optimizations
        if metadata.mime_type.startswith("image/"):
            optimizations["image_optimization"] = await self._optimize_image(content_id)

        elif metadata.mime_type.startswith("video/"):
            optimizations["video_optimization"] = await self._optimize_video(content_id)

        # Update cache headers
        optimizations["cache_optimization"] = await self._optimize_cache_headers(
            content_id, optimization_type
        )

        # Update metadata
        metadata.compression = list(set(metadata.compression + optimizations["compression_applied"]))
        await self._update_metadata_in_db(metadata)

        logger.info(f"âš¡ Content optimized: {content_id}")
        return optimizations

    async def prefetch_content(
        self,
        content_ids: List[str],
        target_regions: List[str]
    ) -> Dict[str, bool]:
        """Prefetch content to specific regions for faster access"""

        results = {}

        for content_id in content_ids:
            if content_id not in self.content_metadata:
                results[content_id] = False
                continue

            metadata = self.content_metadata[content_id]

            for region in target_regions:
                success = await self._prefetch_to_region(content_id, metadata, region)
                results[f"{content_id}:{region}"] = success

        logger.info(f"ðŸš€ Prefetched {len(content_ids)} content items to {len(target_regions)} regions")
        return results

    async def invalidate_cache(
        self,
        content_ids: List[str],
        regions: Optional[List[str]] = None
    ) -> Dict[str, bool]:
        """Invalidate CDN cache for updated content"""

        results = {}

        for content_id in content_ids:
            if content_id not in self.content_metadata:
                results[content_id] = False
                continue

            metadata = self.content_metadata[content_id]

            # Invalidate in all regions or specified regions
            regions_to_invalidate = regions or list(metadata.cdn_urls.keys())

            for region in regions_to_invalidate:
                if region in metadata.cdn_urls:
                    success = await self._invalidate_endpoint_cache(
                        content_id, metadata, region
                    )
                    results[f"{content_id}:{region}"] = success

        logger.info(f"ðŸ”„ Cache invalidated for {len(content_ids)} content items")
        return results

    async def get_cdn_metrics(
        self,
        time_range_hours: int = 24,
        group_by: str = "endpoint"
    ) -> Dict[str, Any]:
        """Get comprehensive CDN performance metrics"""

        cutoff_time = datetime.utcnow() - timedelta(hours=time_range_hours)

        # Filter recent metrics
        recent_metrics = []
        for endpoint_metrics in self.metrics_data.values():
            recent_metrics.extend([
                m for m in endpoint_metrics
                if m.timestamp >= cutoff_time
            ])

        # Aggregate metrics
        if group_by == "endpoint":
            aggregated = await self._aggregate_metrics_by_endpoint(recent_metrics)
        elif group_by == "region":
            aggregated = await self._aggregate_metrics_by_region(recent_metrics)
        elif group_by == "content_type":
            aggregated = await self._aggregate_metrics_by_content_type(recent_metrics)
        else:
            aggregated = await self._aggregate_metrics_global(recent_metrics)

        # Calculate overall performance
        overall_performance = {
            "total_requests": sum(m.requests_total for m in recent_metrics),
            "total_bandwidth_gb": sum(m.bandwidth_used_gb for m in recent_metrics),
            "average_cache_hit_rate": sum(m.cache_hit_rate for m in recent_metrics) / len(recent_metrics),
            "average_latency_ms": sum(m.average_latency_ms for m in recent_metrics) / len(recent_metrics),
            "overall_error_rate": sum(m.error_rate for m in recent_metrics) / len(recent_metrics),
            "cache_efficiency": self.cache_hits / (self.cache_hits + self.cache_misses) if (self.cache_hits + self.cache_misses) > 0 else 0
        }

        return {
            "time_range_hours": time_range_hours,
            "group_by": group_by,
            "aggregated_metrics": aggregated,
            "overall_performance": overall_performance,
            "generated_at": datetime.utcnow().isoformat()
        }

    async def setup_edge_computing(
        self,
        content_id: str,
        edge_function: str,
        regions: List[str]
    ) -> bool:
        """Setup edge computing for dynamic content"""

        if content_id not in self.content_metadata:
            return False

        # Deploy edge function to specified regions
        deployment_results = {}

        for region in regions:
            success = await self._deploy_edge_function(content_id, edge_function, region)
            deployment_results[region] = success

        success_rate = sum(deployment_results.values()) / len(deployment_results)

        logger.info(f"ðŸ”§ Edge computing setup for {content_id}: {success_rate:.1%} success rate")
        return success_rate > 0.8

    # Private helper methods

    async def _initialize_cdn_endpoints(self):
        """Initialize CDN endpoints"""

        # Example endpoint configurations (in production, load from config/database)
        endpoints = [
            CDNEndpoint(
                endpoint_id="aws-us-east",
                provider=CDNProvider.CLOUDFRONT,
                region="us-east-1",
                base_url="https://cdn-us-east.eduverse.com",
                access_key=os.getenv("AWS_ACCESS_KEY_ID", ""),
                secret_key=os.getenv("AWS_SECRET_ACCESS_KEY", ""),
                distribution_id="EDUVERSE-US-EAST"
            ),
            CDNEndpoint(
                endpoint_id="aws-eu-west",
                provider=CDNProvider.CLOUDFRONT,
                region="eu-west-1",
                base_url="https://cdn-eu-west.eduverse.com",
                access_key=os.getenv("AWS_ACCESS_KEY_ID", ""),
                secret_key=os.getenv("AWS_SECRET_ACCESS_KEY", ""),
                distribution_id="EDUVERSE-EU-WEST"
            ),
            CDNEndpoint(
                endpoint_id="azure-asia-east",
                provider=CDNProvider.AZURE_CDN,
                region="asia-east-1",
                base_url="https://cdn-asia-east.eduverse.com",
                access_key=os.getenv("AZURE_ACCESS_KEY", ""),
                secret_key=os.getenv("AZURE_SECRET_KEY", "")
            )
        ]

        for endpoint in endpoints:
            self.endpoints[endpoint.endpoint_id] = endpoint

        logger.info(f"ðŸŒ Initialized {len(endpoints)} CDN endpoints")

    def _initialize_cache_strategies(self):
        """Initialize caching strategies for different content types"""

        self.cache_strategy = {
            ContentType.STATIC_ASSETS: {
                "max_age": 86400 * 30,  # 30 days
                "compression": ["gzip", "brotli"],
                "optimization": ["minify", "image_optimize"],
                "invalidation": "rare"
            },
            ContentType.USER_UPLOADS: {
                "max_age": 86400 * 7,  # 7 days
                "compression": ["gzip"],
                "optimization": ["image_optimize"],
                "invalidation": "user_specific"
            },
            ContentType.COURSE_CONTENT: {
                "max_age": 86400 * 14,  # 14 days
                "compression": ["gzip", "brotli"],
                "optimization": ["video_optimize"],
                "invalidation": "version_based"
            },
            ContentType.VIDEO_STREAMING: {
                "max_age": 86400 * 3,  # 3 days
                "compression": ["gzip"],
                "optimization": ["video_transcode", "streaming_optimize"],
                "invalidation": "immediate"
            },
            ContentType.VR_AR_ASSETS: {
                "max_age": 86400 * 60,  # 60 days
                "compression": ["gzip", "brotli"],
                "optimization": ["3d_optimize", "texture_compress"],
                "invalidation": "rare"
            },
            ContentType.AI_GENERATED: {
                "max_age": 86400 * 90,  # 90 days
                "compression": ["gzip", "brotli"],
                "optimization": ["ai_specific"],
                "invalidation": "version_based"
            }
        }

    async def _setup_content_optimization(self):
        """Setup content optimization pipelines"""

        # Initialize optimization services
        self.image_optimizer = ImageOptimizer()
        self.video_optimizer = VideoOptimizer()
        self.compression_engine = CompressionEngine()

        logger.info("âš™ï¸ Content optimization pipelines initialized")

    async def _upload_to_endpoint(
        self,
        file_path: str,
        endpoint: CDNEndpoint,
        content_id: str,
        custom_filename: Optional[str],
        content_type: ContentType
    ) -> str:
        """Upload file to specific CDN endpoint"""

        try:
            if endpoint.provider == CDNProvider.CLOUDFRONT:
                return await self._upload_to_cloudfront(file_path, endpoint, content_id, custom_filename)
            elif endpoint.provider == CDNProvider.AZURE_CDN:
                return await self._upload_to_azure(file_path, endpoint, content_id, custom_filename)
            elif endpoint.provider == CDNProvider.CLOUDFLARE:
                return await self._upload_to_cloudflare(file_path, endpoint, content_id, custom_filename)
            else:
                # Generic upload method
                return await self._generic_upload(file_path, endpoint, content_id, custom_filename)

        except Exception as e:
            logger.error(f"âŒ Upload failed to {endpoint.endpoint_id}: {e}")
            raise

    async def _upload_to_cloudfront(self, file_path: str, endpoint: CDNEndpoint, content_id: str, custom_filename: str) -> str:
        """Upload to AWS CloudFront"""

        # Initialize S3 client for CloudFront
        s3_client = boto3.client(
            's3',
            aws_access_key_id=endpoint.access_key,
            aws_secret_access_key=endpoint.secret_key,
            region_name=endpoint.region
        )

        # Generate filename
        filename = custom_filename or os.path.basename(file_path)
        s3_key = f"{content_id}/{filename}"

        # Upload to S3 (CloudFront serves from S3)
        with open(file_path, 'rb') as file_data:
            s3_client.upload_fileobj(file_data, endpoint.bucket_name, s3_key)

        # Generate CloudFront URL
        cdn_url = f"{endpoint.base_url}/{s3_key}"

        logger.debug(f"â˜ï¸ Uploaded to CloudFront: {cdn_url}")
        return cdn_url

    async def _upload_to_azure(self, file_path: str, endpoint: CDNEndpoint, content_id: str, custom_filename: str) -> str:
        """Upload to Azure CDN"""

        # Implementation for Azure Blob Storage upload
        cdn_url = f"{endpoint.base_url}/{content_id}/{custom_filename or os.path.basename(file_path)}"

        logger.debug(f"ðŸ”µ Uploaded to Azure CDN: {cdn_url}")
        return cdn_url

    async def _upload_to_cloudflare(self, file_path: str, endpoint: CDNEndpoint, content_id: str, custom_filename: str) -> str:
        """Upload to Cloudflare"""

        # Implementation for Cloudflare R2 or KV storage
        cdn_url = f"{endpoint.base_url}/{content_id}/{custom_filename or os.path.basename(file_path)}"

        logger.debug(f"ðŸŸ  Uploaded to Cloudflare: {cdn_url}")
        return cdn_url

    async def _generic_upload(self, file_path: str, endpoint: CDNEndpoint, content_id: str, custom_filename: str) -> str:
        """Generic upload method"""

        cdn_url = f"{endpoint.base_url}/{content_id}/{custom_filename or os.path.basename(file_path)}"

        logger.debug(f"ðŸ“¤ Generic upload: {cdn_url}")
        return cdn_url

    async def _generate_file_checksum(self, file_path: str) -> str:
        """Generate SHA-256 checksum for file"""

        sha256_hash = hashlib.sha256()

        async with aiofiles.open(file_path, 'rb') as file:
            # Read file in chunks for large files
            while chunk := await file.read(8192):
                sha256_hash.update(chunk)

        return sha256_hash.hexdigest()

    def _get_compression_methods(self, file_path: str) -> List[str]:
        """Determine which compression methods to apply"""

        mime_type, _ = mimetypes.guess_type(file_path)

        if mime_type and mime_type.startswith('text/'):
            return ['gzip', 'brotli']
        elif mime_type and mime_type.startswith('image/'):
            return ['gzip']  # Images are already compressed
        elif mime_type and mime_type.startswith('video/'):
            return ['gzip']  # Video compression handled separately
        else:
            return ['gzip']

    def _generate_cache_headers(self, content_type: ContentType) -> Dict[str, str]:
        """Generate appropriate cache headers"""

        strategy = self.cache_strategy[content_type]

        headers = {
            "Cache-Control": f"max-age={strategy['max_age']}",
            "CDN-Cache-Control": f"max-age={strategy['max_age']}"
        }

        if strategy['max_age'] > 86400:  # Long-term caching
            headers["Cache-Control"] += ", public, immutable"

        return headers

    async def _detect_user_region(self) -> str:
        """Detect user's geographic region"""

        # In production, use geo-IP services
        # For now, return a default region
        return "us-east-1"

    async def _select_optimal_endpoint(
        self,
        cdn_urls: Dict[str, str],
        user_region: str,
        preferred_provider: Optional[CDNProvider]
    ) -> str:
        """Select optimal CDN endpoint for user"""

        # Priority 1: User's region
        if user_region in cdn_urls:
            return cdn_urls[user_region]

        # Priority 2: Preferred provider
        if preferred_provider:
            for region, url in cdn_urls.items():
                endpoint = self.endpoints.get(f"{preferred_provider.value}-{region}")
                if endpoint:
                    return url

        # Priority 3: Closest region (simple logic)
        region_priority = {
            "us-east-1": 1,
            "us-west-1": 2,
            "eu-west-1": 3,
            "asia-east-1": 4
        }

        # Sort regions by priority
        sorted_regions = sorted(cdn_urls.keys(), key=lambda r: region_priority.get(r, 999))

        return cdn_urls[sorted_regions[0]] if sorted_regions else list(cdn_urls.values())[0]

    async def _record_content_access(self, content_id: str, region: str):
        """Record content access for analytics"""

        # Update access metrics (in production, use proper metrics collection)
        self.request_count += 1

    async def _apply_compression(self, content_id: str, compression_type: str):
        """Apply compression to content"""

        metadata = self.content_metadata[content_id]

        # Apply compression based on type
        if compression_type == "gzip":
            await self._apply_gzip_compression(content_id)
        elif compression_type == "brotli":
            await self._apply_brotli_compression(content_id)

        logger.debug(f"ðŸ—œï¸ Applied {compression_type} compression to {content_id}")

    async def _apply_gzip_compression(self, content_id: str):
        """Apply gzip compression"""

        # Implementation for gzip compression
        pass

    async def _apply_brotli_compression(self, content_id: str):
        """Apply brotli compression"""

        # Implementation for brotli compression
        pass

    async def _optimize_image(self, content_id: str) -> Dict[str, Any]:
        """Optimize image content"""

        metadata = self.content_metadata[content_id]

        optimizations = {
            "format_conversion": "webp",
            "quality_reduction": 85,
            "size_reduction_percent": 25,
            "responsive_images": True
        }

        # Apply image optimizations
        # Implementation would use image processing libraries

        return optimizations

    async def _optimize_video(self, content_id: str) -> Dict[str, Any]:
        """Optimize video content"""

        metadata = self.content_metadata[content_id]

        optimizations = {
            "format_conversion": "hls",
            "bitrate_optimization": True,
            "resolution_adaptation": True,
            "segment_duration": 10
        }

        # Apply video optimizations
        # Implementation would use video processing libraries

        return optimizations

    async def _optimize_cache_headers(self, content_id: str, optimization_type: str) -> Dict[str, str]:
        """Optimize cache headers"""

        metadata = self.content_metadata[content_id]

        # Adjust cache headers based on optimization type
        if optimization_type == "aggressive":
            return {
                "Cache-Control": "max-age=86400, public, immutable",
                "CDN-Cache-Control": "max-age=86400"
            }
        elif optimization_type == "moderate":
            return {
                "Cache-Control": "max-age=3600, public",
                "CDN-Cache-Control": "max-age=3600"
            }
        else:
            return metadata.cache_headers

    async def _prefetch_to_region(self, content_id: str, metadata: ContentMetadata, region: str) -> bool:
        """Prefetch content to specific region"""

        if region not in metadata.cdn_urls:
            return False

        # Implementation for prefetching
        # This would trigger CDN prefetch/invalidation APIs

        logger.debug(f"âš¡ Prefetched {content_id} to {region}")
        return True

    async def _invalidate_endpoint_cache(self, content_id: str, metadata: ContentMetadata, region: str) -> bool:
        """Invalidate cache for specific endpoint"""

        # Implementation for cache invalidation
        # This would call CDN invalidation APIs

        logger.debug(f"ðŸ”„ Invalidated cache for {content_id} in {region}")
        return True

    async def _deploy_edge_function(self, content_id: str, function_code: str, region: str) -> bool:
        """Deploy edge function to region"""

        # Implementation for edge function deployment
        # This would deploy to CDN edge locations

        logger.debug(f"ðŸš€ Deployed edge function for {content_id} to {region}")
        return True

    async def _store_metadata_in_db(self, metadata: ContentMetadata):
        """Store content metadata in database"""
        # Implementation for database storage
        pass

    async def _update_metadata_in_db(self, metadata: ContentMetadata):
        """Update content metadata in database"""
        # Implementation for database update
        pass

    async def _aggregate_metrics_by_endpoint(self, metrics: List[CDNMetrics]) -> Dict[str, Any]:
        """Aggregate metrics by endpoint"""

        endpoint_metrics = {}

        for metric in metrics:
            if metric.endpoint_id not in endpoint_metrics:
                endpoint_metrics[metric.endpoint_id] = {
                    "requests_total": 0,
                    "bandwidth_total": 0,
                    "latency_sum": 0,
                    "count": 0
                }

            endpoint_metrics[metric.endpoint_id]["requests_total"] += metric.requests_total
            endpoint_metrics[metric.endpoint_id]["bandwidth_total"] += metric.bandwidth_used_gb
            endpoint_metrics[metric.endpoint_id]["latency_sum"] += metric.average_latency_ms
            endpoint_metrics[metric.endpoint_id]["count"] += 1

        # Calculate averages
        for endpoint_id, data in endpoint_metrics.items():
            if data["count"] > 0:
                data["average_latency"] = data["latency_sum"] / data["count"]
                data["requests_per_second"] = data["requests_total"] / (len(metrics) * 3600)  # Assuming hourly data
            del data["latency_sum"], data["count"]

        return endpoint_metrics

    async def _aggregate_metrics_by_region(self, metrics: List[CDNMetrics]) -> Dict[str, Any]:
        """Aggregate metrics by region"""

        region_metrics = {}

        for metric in metrics:
            if metric.region not in region_metrics:
                region_metrics[metric.region] = {
                    "requests_total": 0,
                    "bandwidth_total": 0,
                    "latency_sum": 0,
                    "count": 0
                }

            region_metrics[metric.region]["requests_total"] += metric.requests_total
            region_metrics[metric.region]["bandwidth_total"] += metric.bandwidth_used_gb
            region_metrics[metric.region]["latency_sum"] += metric.average_latency_ms
            region_metrics[metric.region]["count"] += 1

        # Calculate averages
        for region, data in region_metrics.items():
            if data["count"] > 0:
                data["average_latency"] = data["latency_sum"] / data["count"]
            del data["latency_sum"], data["count"]

        return region_metrics

    async def _aggregate_metrics_by_content_type(self, metrics: List[CDNMetrics]) -> Dict[str, Any]:
        """Aggregate metrics by content type"""

        # This would require content type information in metrics
        # For now, return global aggregation
        return await self._aggregate_metrics_global(metrics)

    async def _aggregate_metrics_global(self, metrics: List[CDNMetrics]) -> Dict[str, Any]:
        """Aggregate metrics globally"""

        if not metrics:
            return {}

        return {
            "total_requests": sum(m.requests_total for m in metrics),
            "total_bandwidth_gb": sum(m.bandwidth_used_gb for m in metrics),
            "average_cache_hit_rate": sum(m.cache_hit_rate for m in metrics) / len(metrics),
            "average_latency_ms": sum(m.average_latency_ms for m in metrics) / len(metrics),
            "average_error_rate": sum(m.error_rate for m in metrics) / len(metrics)
        }

    async def get_content_performance(
        self,
        content_id: str,
        time_range_hours: int = 24
    ) -> Dict[str, Any]:
        """Get performance metrics for specific content"""

        if content_id not in self.content_metadata:
            return {"error": "Content not found"}

        metadata = self.content_metadata[content_id]

        # Get metrics for this content (in production, filter by content_id)
        # For now, return general performance

        return {
            "content_id": content_id,
            "total_requests": self.request_count,
            "cache_hit_rate": self.cache_hits / (self.cache_hits + self.cache_misses) if (self.cache_hits + self.cache_misses) > 0 else 0,
            "average_delivery_time_ms": 150,  # Mock value
            "regions_accessed": list(metadata.cdn_urls.keys()),
            "optimization_applied": metadata.compression,
            "file_size": metadata.file_size,
            "mime_type": metadata.mime_type
        }

    async def optimize_for_bandwidth(
        self,
        content_id: str,
        target_bandwidth: str
    ) -> Dict[str, Any]:
        """Optimize content for specific bandwidth conditions"""

        if content_id not in self.content_metadata:
            return {"error": "Content not found"}

        metadata = self.content_metadata[content_id]

        optimizations = {}

        if target_bandwidth == "low":
            # Aggressive compression and quality reduction
            optimizations = {
                "compression_level": "maximum",
                "image_quality": 60,
                "video_bitrate": "low",
                "enable_progressive_loading": True
            }
        elif target_bandwidth == "medium":
            # Moderate optimization
            optimizations = {
                "compression_level": "standard",
                "image_quality": 75,
                "video_bitrate": "medium",
                "enable_progressive_loading": True
            }
        else:
            # High bandwidth - minimal optimization
            optimizations = {
                "compression_level": "minimum",
                "image_quality": 90,
                "video_bitrate": "high",
                "enable_progressive_loading": False
            }

        # Apply optimizations
        await self._apply_bandwidth_optimizations(content_id, optimizations)

        return optimizations

    async def _apply_bandwidth_optimizations(self, content_id: str, optimizations: Dict[str, Any]):
        """Apply bandwidth-specific optimizations"""

        # Implementation for bandwidth optimization
        pass

    async def setup_geo_replication(
        self,
        content_id: str,
        replication_regions: List[str]
    ) -> bool:
        """Setup geographic replication for content"""

        if content_id not in self.content_metadata:
            return False

        metadata = self.content_metadata[content_id]

        # Replicate to specified regions
        replication_tasks = []

        for region in replication_regions:
            if region not in metadata.cdn_urls:
                task = self._replicate_to_region(content_id, metadata, region)
                replication_tasks.append(task)

        # Wait for replication completion
        results = await asyncio.gather(*replication_tasks, return_exceptions=True)

        success_rate = sum(1 for r in results if not isinstance(r, Exception)) / len(results)

        logger.info(f"ðŸŒ Geo-replication completed for {content_id}: {success_rate:.1%} success rate")
        return success_rate > 0.8

    async def _replicate_to_region(self, content_id: str, metadata: ContentMetadata, region: str) -> bool:
        """Replicate content to specific region"""

        # Implementation for content replication
        return True

    async def get_cdn_health_status(self) -> Dict[str, Any]:
        """Get health status of all CDN endpoints"""

        health_status = {}

        for endpoint_id, endpoint in self.endpoints.items():
            # Check endpoint health
            health = await self._check_endpoint_health(endpoint)

            health_status[endpoint_id] = {
                "endpoint_id": endpoint_id,
                "provider": endpoint.provider.value,
                "region": endpoint.region,
                "is_healthy": health["healthy"],
                "latency_ms": health["latency"],
                "error_rate": health["error_rate"],
                "last_checked": datetime.utcnow().isoformat()
            }

        return {
            "overall_health": all(status["is_healthy"] for status in health_status.values()),
            "endpoints": health_status,
            "total_endpoints": len(self.endpoints),
            "healthy_endpoints": len([s for s in health_status.values() if s["is_healthy"]])
        }

    async def _check_endpoint_health(self, endpoint: CDNEndpoint) -> Dict[str, Any]:
        """Check health of specific endpoint"""

        # Implementation for health checking
        # This would ping the endpoint and measure response time

        return {
            "healthy": True,
            "latency": 45,  # Mock latency in ms
            "error_rate": 0.001  # Mock error rate
        }

    async def cleanup(self):
        """Cleanup CDN resources"""
        logger.info("âœ… Global CDN service cleaned up")


class ImageOptimizer:
    """Advanced image optimization service"""

    def __init__(self):
        self.supported_formats = ['jpeg', 'png', 'webp', 'avif']
        self.optimization_levels = {
            'low': {'quality': 60, 'progressive': False},
            'medium': {'quality': 75, 'progressive': True},
            'high': {'quality': 90, 'progressive': True},
            'lossless': {'quality': 100, 'progressive': True}
        }

    async def optimize_image(
        self,
        image_path: str,
        optimization_level: str = 'medium',
        target_format: str = 'auto'
    ) -> Dict[str, Any]:
        """Optimize image for web delivery"""

        # Implementation for image optimization
        # Would use libraries like Pillow, ImageMagick, or cloud services

        return {
            "original_size": os.path.getsize(image_path),
            "optimized_size": os.path.getsize(image_path) * 0.7,  # Mock 30% reduction
            "format": target_format,
            "quality": self.optimization_levels[optimization_level]['quality'],
            "dimensions": "1920x1080"  # Mock dimensions
        }


class VideoOptimizer:
    """Advanced video optimization service"""

    def __init__(self):
        self.supported_formats = ['mp4', 'webm', 'hls', 'dash']
        self.bitrate_profiles = {
            'low': '500k',
            'medium': '1200k',
            'high': '2400k',
            'ultra': '4000k'
        }

    async def optimize_video(
        self,
        video_path: str,
        target_bitrate: str = 'medium',
        enable_streaming: bool = True
    ) -> Dict[str, Any]:
        """Optimize video for streaming"""

        # Implementation for video optimization
        # Would use libraries like FFmpeg

        return {
            "original_size": os.path.getsize(video_path),
            "optimized_size": os.path.getsize(video_path) * 0.6,  # Mock 40% reduction
            "bitrate": self.bitrate_profiles[target_bitrate],
            "streaming_enabled": enable_streaming,
            "segments": 120 if enable_streaming else 1
        }


class CompressionEngine:
    """Advanced compression engine"""

    def __init__(self):
        self.compression_algorithms = {
            'gzip': self._gzip_compress,
            'brotli': self._brotli_compress,
            'lz4': self._lz4_compress
        }

    async def compress_file(
        self,
        file_path: str,
        algorithm: str = 'gzip',
        compression_level: int = 6
    ) -> bytes:
        """Compress file using specified algorithm"""

        if algorithm not in self.compression_algorithms:
            raise ValueError(f"Unsupported compression algorithm: {algorithm}")

        async with aiofiles.open(file_path, 'rb') as file:
            data = await file.read()

        return await self.compression_algorithms[algorithm](data, compression_level)

    async def _gzip_compress(self, data: bytes, level: int) -> bytes:
        """Apply gzip compression"""
        import gzip
        import io

        buffer = io.BytesIO()
        with gzip.GzipFile(fileobj=buffer, mode='wb', compresslevel=level) as f:
            f.write(data)

        return buffer.getvalue()

    async def _brotli_compress(self, data: bytes, level: int) -> bytes:
        """Apply brotli compression"""
        # Implementation for brotli compression
        return data  # Placeholder

    async def _lz4_compress(self, data: bytes, level: int) -> bytes:
        """Apply LZ4 compression"""
        # Implementation for LZ4 compression
        return data  # Placeholder


class CDNMonitor:
    """Monitor CDN performance and health"""

    def __init__(self, cdn_service: GlobalCDNService):
        self.cdn_service = cdn_service
        self.monitoring_interval = 60  # seconds
        self.monitoring_task: Optional[asyncio.Task] = None

    async def start_monitoring(self):
        """Start CDN monitoring"""

        self.monitoring_task = asyncio.create_task(self._monitoring_loop())
        logger.info("ðŸ“Š CDN monitoring started")

    async def stop_monitoring(self):
        """Stop CDN monitoring"""

        if self.monitoring_task:
            self.monitoring_task.cancel()
        logger.info("â¹ï¸ CDN monitoring stopped")

    async def _monitoring_loop(self):
        """Main monitoring loop"""

        while True:
            try:
                await asyncio.sleep(self.monitoring_interval)

                # Collect metrics from all endpoints
                for endpoint_id, endpoint in self.cdn_service.endpoints.items():
                    if endpoint.is_active:
                        metrics = await self._collect_endpoint_metrics(endpoint)
                        if endpoint_id not in self.cdn_service.metrics_data:
                            self.cdn_service.metrics_data[endpoint_id] = []
                        self.cdn_service.metrics_data[endpoint_id].append(metrics)

                        # Keep only last 1000 metrics per endpoint
                        if len(self.cdn_service.metrics_data[endpoint_id]) > 1000:
                            self.cdn_service.metrics_data[endpoint_id] = self.cdn_service.metrics_data[endpoint_id][-1000:]

                # Check for performance issues
                await self._check_performance_anomalies()

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"âŒ Monitoring error: {e}")

    async def _collect_endpoint_metrics(self, endpoint: CDNEndpoint) -> CDNMetrics:
        """Collect metrics for specific endpoint"""

        # In production, this would query CDN APIs for real metrics
        # For now, return mock metrics

        return CDNMetrics(
            endpoint_id=endpoint.endpoint_id,
            timestamp=datetime.utcnow(),
            requests_total=1000,  # Mock
            requests_per_second=2.5,  # Mock
            bandwidth_used_gb=10.5,  # Mock
            cache_hit_rate=0.85,  # Mock
            average_latency_ms=45,  # Mock
            error_rate=0.001,  # Mock
            region=endpoint.region
        )

    async def _check_performance_anomalies(self):
        """Check for performance anomalies"""

        # Analyze recent metrics for anomalies
        # Implementation would use statistical analysis

        pass


class CDNLoadBalancer:
    """Intelligent load balancer for CDN traffic"""

    def __init__(self, cdn_service: GlobalCDNService):
        self.cdn_service = cdn_service
        self.load_balancing_strategy = "weighted_round_robin"
        self.endpoint_weights: Dict[str, float] = {}

    async def select_optimal_endpoint(
        self,
        user_region: str,
        content_type: ContentType,
        user_bandwidth: str = "high"
    ) -> str:
        """Select optimal endpoint based on multiple factors"""

        # Calculate weights for each endpoint
        for endpoint_id, endpoint in self.cdn_service.endpoints.items():
            if not endpoint.is_active:
                continue

            weight = await self._calculate_endpoint_weight(
                endpoint, user_region, content_type, user_bandwidth
            )
            self.endpoint_weights[endpoint_id] = weight

        # Select endpoint with highest weight
        if self.endpoint_weights:
            best_endpoint_id = max(self.endpoint_weights, key=self.endpoint_weights.get)
            return best_endpoint_id

        # Fallback to first available endpoint
        for endpoint_id, endpoint in self.cdn_service.endpoints.items():
            if endpoint.is_active:
                return endpoint_id

        raise Exception("No active CDN endpoints available")

    async def _calculate_endpoint_weight(
        self,
        endpoint: CDNEndpoint,
        user_region: str,
        content_type: ContentType,
        user_bandwidth: str
    ) -> float:
        """Calculate weight for endpoint selection"""

        weight = 1.0

        # Geographic proximity (40% weight)
        if endpoint.region.startswith(user_region.split('-')[0]):
            weight *= 1.4
        else:
            # Distance penalty
            weight *= 0.8

        # Performance metrics (30% weight)
        recent_metrics = self.cdn_service.metrics_data.get(endpoint.endpoint_id, [])
        if recent_metrics:
            latest_metrics = recent_metrics[-1]
            # Higher cache hit rate = higher weight
            weight *= (0.7 + latest_metrics.cache_hit_rate * 0.3)

        # Content type optimization (20% weight)
        if content_type == ContentType.VIDEO_STREAMING and "streaming" in endpoint.base_url:
            weight *= 1.2

        # Bandwidth compatibility (10% weight)
        if user_bandwidth == "low" and "optimized" in endpoint.base_url:
            weight *= 1.1

        return weight
